{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "26d4d114",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hospital Clínic', 'Hospital Josep Trueta', 'Hospital Sant Pau', 'Hospital Vall Hebrón', 'README', 'all_data_inc_trueta.xlsx']\n"
     ]
    }
   ],
   "source": [
    "import os \n",
    "import numpy as np\n",
    "\n",
    "path = \"/media/cansu/DiskSpace/Cansu/HE_Prediction/RAINS_vicorob\"\n",
    "all_files = os.listdir(path)\n",
    "all_files.sort()\n",
    "print(all_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "64a5c762",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Hospital name</th>\n",
       "      <th>id</th>\n",
       "      <th>Basal volume, ml</th>\n",
       "      <th>FU volume, ml</th>\n",
       "      <th>Absolute vol diff, ml</th>\n",
       "      <th>Relative vol diff (FU_vol/Basal_vol)</th>\n",
       "      <th>HE vicorob</th>\n",
       "      <th>HE real</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Hospital Clínic</td>\n",
       "      <td>2098</td>\n",
       "      <td>5.02</td>\n",
       "      <td>4.87</td>\n",
       "      <td>-0.150</td>\n",
       "      <td>0.970</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Hospital Clínic</td>\n",
       "      <td>233632</td>\n",
       "      <td>36.07</td>\n",
       "      <td>39.24</td>\n",
       "      <td>3.171</td>\n",
       "      <td>1.088</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Hospital Clínic</td>\n",
       "      <td>261065</td>\n",
       "      <td>9.50</td>\n",
       "      <td>11.41</td>\n",
       "      <td>1.915</td>\n",
       "      <td>1.202</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Hospital Clínic</td>\n",
       "      <td>34333</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Hospital Clínic</td>\n",
       "      <td>397280</td>\n",
       "      <td>9.37</td>\n",
       "      <td>8.33</td>\n",
       "      <td>-1.038</td>\n",
       "      <td>0.889</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Hospital name      id  Basal volume, ml  FU volume, ml  \\\n",
       "0  Hospital Clínic    2098              5.02           4.87   \n",
       "1  Hospital Clínic  233632             36.07          39.24   \n",
       "2  Hospital Clínic  261065              9.50          11.41   \n",
       "3  Hospital Clínic   34333               NaN            NaN   \n",
       "4  Hospital Clínic  397280              9.37           8.33   \n",
       "\n",
       "   Absolute vol diff, ml  Relative vol diff (FU_vol/Basal_vol)  HE vicorob  \\\n",
       "0                 -0.150                                 0.970         0.0   \n",
       "1                  3.171                                 1.088         0.0   \n",
       "2                  1.915                                 1.202         0.0   \n",
       "3                    NaN                                   NaN         NaN   \n",
       "4                 -1.038                                 0.889         0.0   \n",
       "\n",
       "   HE real  \n",
       "0      NaN  \n",
       "1      NaN  \n",
       "2      NaN  \n",
       "3      NaN  \n",
       "4      NaN  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "excel_file = \"/media/cansu/DiskSpace/Cansu/HE_Prediction/RAINS_vicorob/all_data_inc_trueta.xlsx\"\n",
    "df = pd.read_excel(excel_file)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5163cb6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated dataframe saved to /media/cansu/DiskSpace/Cansu/HE_Prediction/RAINS_vicorob/all_data_inc_trueta.xlsx\n"
     ]
    }
   ],
   "source": [
    "# for all the cases of hospital sant pau, make their ids 3 digits by adding leading zeros\n",
    "df.loc[df['Hospital name'] == 'Hospital Sant Pau', 'id'] = df.loc[df['Hospital name'] == 'Hospital Sant Pau', 'id'].apply(lambda x: str(x).zfill(3))\n",
    "df.loc[df['Hospital name'] == 'Hospital Vall Hebrón', 'id'] = df.loc[df['Hospital name'] == 'Hospital Vall Hebrón', 'id'].apply(lambda x: str(x).zfill(3))\n",
    "df.head()\n",
    "\n",
    "# save updated dataframe to excel file\n",
    "output_excel = \"/media/cansu/DiskSpace/Cansu/HE_Prediction/RAINS_vicorob/all_data_inc_trueta.xlsx\"\n",
    "df.to_excel(output_excel, index=False)\n",
    "print(f\"Updated dataframe saved to {output_excel}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1c44cb2d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hospital Clínic',\n",
       " 'Hospital Josep Trueta',\n",
       " 'Hospital Sant Pau',\n",
       " 'Hospital Vall Hebrón',\n",
       " 'README',\n",
       " 'all_data_inc_trueta.xlsx']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "70015fb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing entries (hospital, id, has_basal, has_fu): 24\n",
      "[('Hospital Clínic', '34333', True, False), ('Hospital Clínic', '4084062', True, False), ('Hospital Clínic', '4731571', True, False), ('Hospital Clínic', '5229590', True, False), ('Hospital Clínic', '5281637', True, False), ('Hospital Clínic', '5295375', True, False), ('Hospital Clínic', '600801', True, False), ('Hospital Clínic', '70293754', True, False), ('Hospital Clínic', '70588150', True, False), ('Hospital Sant Pau', '016', True, False), ('Hospital Sant Pau', '033', True, False), ('Hospital Vall Hebrón', '016', True, False), ('Hospital Vall Hebrón', '017', True, False), ('Hospital Vall Hebrón', '023', True, False), ('Hospital Vall Hebrón', '053', True, False), ('Hospital Vall Hebrón', '054', True, False), ('Hospital Vall Hebrón', '056', False, True), ('Hospital Vall Hebrón', '097', True, False), ('Hospital Vall Hebrón', '121', True, False), ('Hospital Vall Hebrón', '126', True, False), ('Hospital Vall Hebrón', '139', True, False), ('Hospital Vall Hebrón', '165', True, False), ('Hospital Vall Hebrón', '171', True, False), ('Hospital Vall Hebrón', '178', True, False)]\n",
      "Saved CSV with paths to all_data_with_paths.csv\n"
     ]
    }
   ],
   "source": [
    "# create a csv file to read the data easily.\n",
    "# For each patient find Basal and FU paths and save to df columns\n",
    "\n",
    "# add 2 new columns for the paths\n",
    "df['basal_img_path'] = ''\n",
    "df['basal_mask_path'] = ''\n",
    "df['fu_img_path'] = ''\n",
    "df['fu_mask_path'] = ''\n",
    "\n",
    "base_path = \"/media/cansu/DiskSpace/Cansu/HE_Prediction/RAINS_vicorob\"\n",
    "\n",
    "def collect_files(hospital_dir):\n",
    "    files = []\n",
    "    for root, _, filenames in os.walk(hospital_dir):\n",
    "        for fname in filenames:\n",
    "            if fname.lower().endswith(('.nii', '.nii.gz', '.mha', '.nrrd')):\n",
    "                files.append(os.path.join(root, fname))\n",
    "    return files\n",
    "\n",
    "missing = []\n",
    "for hospital in df['Hospital name'].unique():\n",
    "    hosp_dir = os.path.join(base_path, hospital)\n",
    "    hospital_df = df[df['Hospital name'] == hospital]\n",
    "\n",
    "    for index, row in hospital_df.iterrows():\n",
    "        pid = str(row['id'])\n",
    "        basal_path = os.path.join(hosp_dir, pid, 'Basal')\n",
    "        fu_path = os.path.join(hosp_dir, pid, 'FU1')\n",
    "\n",
    "        basal_image = os.path.join(basal_path, f\"CT_SS.nii.gz\")\n",
    "        basal_mask = os.path.join(basal_path, f\"hematoma_mask_vicorob_reviewed_reoriented.nii.gz\")\n",
    "        fu_image = os.path.join(fu_path, f\"CT_SS.nii.gz\")\n",
    "        fu_mask = os.path.join(fu_path, f\"hematoma_mask_vicorob_reviewed_reoriented.nii.gz\")\n",
    "\n",
    "        # record paths (empty string if not found)\n",
    "        df.at[index, 'basal_img_path'] = basal_image if os.path.exists(basal_image) else ''\n",
    "        df.at[index, 'basal_mask_path'] = basal_mask if os.path.exists(basal_mask) else ''\n",
    "        df.at[index, 'fu_img_path'] = fu_image if os.path.exists(fu_image) else ''\n",
    "        df.at[index, 'fu_mask_path'] = fu_mask if os.path.exists(fu_mask) else ''\n",
    "\n",
    "        if not os.path.exists(basal_image) or not os.path.exists(fu_image):\n",
    "            missing.append((hospital, pid, os.path.exists(basal_image), os.path.exists(fu_image)))\n",
    "\n",
    "# summary\n",
    "print(f\"Missing entries (hospital, id, has_basal, has_fu): {len(missing)}\")\n",
    "if missing:\n",
    "    print(missing[:50])\n",
    "\n",
    "# save augmented table\n",
    "df.to_csv('/media/cansu/DiskSpace/Cansu/HE_Prediction/all_data_with_paths.csv', index=False)\n",
    "print('Saved CSV with paths to all_data_with_paths.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0f526b4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved filtered CSV with both basal and fu images to data_with_both_basal_fu.csv\n"
     ]
    }
   ],
   "source": [
    "# filter the ones which have both basal and fu images\n",
    "df_filtered = df[(df['basal_img_path'] != '') & (df['fu_img_path'] != '') & (df['basal_mask_path'] != '') & (df['fu_mask_path'] != '')]\n",
    "df_filtered.to_csv('/media/cansu/DiskSpace/Cansu/HE_Prediction/data_with_both_basal_fu.csv', index=False)\n",
    "print('Saved filtered CSV with both basal and fu images to data_with_both_basal_fu.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b5f7a831",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original dataframe entries: 482\n",
      "Filtered dataframe entries: 456\n",
      "Number of entries removed: 26\n"
     ]
    }
   ],
   "source": [
    "# compare the filtered dataframe with the original one and print the number of entries removed\n",
    "print(f\"Original dataframe entries: {len(df)}\")\n",
    "print(f\"Filtered dataframe entries: {len(df_filtered)}\")\n",
    "print(f\"Number of entries removed: {len(df) - len(df_filtered)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fb448eb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entries per hospital in filtered dataframe:\n",
      "Hospital name\n",
      "Hospital Josep Trueta    209\n",
      "Hospital Vall Hebrón     167\n",
      "Hospital Clínic           47\n",
      "Hospital Sant Pau         33\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# number of entries from eachh hospital in the filtered dataframe\n",
    "print(\"Entries per hospital in filtered dataframe:\")\n",
    "print(df_filtered['Hospital name'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b917cb04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hospital: Hospital Clínic\n",
      "HE vicorob\n",
      "0.0    39\n",
      "1.0     8\n",
      "Name: count, dtype: int64\n",
      "Hospital: Hospital Sant Pau\n",
      "HE vicorob\n",
      "0.0    24\n",
      "1.0     9\n",
      "Name: count, dtype: int64\n",
      "Hospital: Hospital Vall Hebrón\n",
      "HE vicorob\n",
      "0.0    137\n",
      "1.0     30\n",
      "Name: count, dtype: int64\n",
      "Hospital: Hospital Josep Trueta\n",
      "HE vicorob\n",
      "0.0    166\n",
      "1.0     43\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# for each hospital count the number of \"he vicorob\" columns, if they are 1. and list numbers based on the hospital name\n",
    "for hospital in df_filtered['Hospital name'].unique():\n",
    "    hosp_df = df_filtered[df_filtered['Hospital name'] == hospital]\n",
    "    he_vicorob_counts = hosp_df['HE vicorob'].value_counts()\n",
    "    print(f\"Hospital: {hospital}\")\n",
    "    print(he_vicorob_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4b9c56c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Remove files with specific name in Hospital Vall Hebrón folder - space cleanup\n",
    "# path_hospital = \"/media/cansu/DiskSpace/Cansu/HE_Prediction/RAINS_vicorob/Hospital Vall Hebrón\"\n",
    "\n",
    "# for cases in os.listdir(path_hospital):\n",
    "#     basal_folder = os.path.join(path_hospital, cases, \"Basal\")\n",
    "#     fu_folder = os.path.join(path_hospital, cases, \"FU1\")\n",
    "\n",
    "#     # remove the files if it contains name hematoma_mask_fold0.nii.gz\n",
    "#     for folder in [basal_folder, fu_folder]:\n",
    "#         if os.path.exists(folder):\n",
    "#             for file in os.listdir(folder):\n",
    "#                 if \"hematoma_mask_fold4.nii.gz\" in file:\n",
    "#                     # print(f\"Removing file: {os.path.join(folder, file)}\")\n",
    "#                     os.remove(os.path.join(folder, file))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "098324d7",
   "metadata": {},
   "source": [
    "# Redesigning the Dataset and Dataloading process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6ef5ee3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved IDs for Hospital Clínic to /media/cansu/DiskSpace/Cansu/HE_prediction_improved/data/Hospital_Clínic_ids.txt\n",
      "Saved IDs for Hospital Sant Pau to /media/cansu/DiskSpace/Cansu/HE_prediction_improved/data/Hospital_Sant_Pau_ids.txt\n",
      "Saved IDs for Hospital Vall Hebrón to /media/cansu/DiskSpace/Cansu/HE_prediction_improved/data/Hospital_Vall_Hebrón_ids.txt\n",
      "Saved IDs for Hospital Josep Trueta to /media/cansu/DiskSpace/Cansu/HE_prediction_improved/data/Hospital_Josep_Trueta_ids.txt\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"/media/cansu/DiskSpace/Cansu/HE_prediction_improved/data/data_with_both_basal_fu.csv\")\n",
    "##\n",
    "# bring all ids to string type and save them as txt file from all hospitals excepts Trueta\n",
    "df['id'] = df['id'].astype(str).str.zfill(3)\n",
    "hospitals = df['Hospital name'].unique().tolist()\n",
    "# hospitals.remove('Hospital Josep Trueta')\n",
    "\n",
    "for hospital in hospitals:\n",
    "    hosp_df = df[df['Hospital name'] == hospital]\n",
    "    ids = hosp_df['id'].tolist()\n",
    "    output_txt = f\"/media/cansu/DiskSpace/Cansu/HE_prediction_improved/data/{hospital.replace(' ', '_')}_ids.txt\"\n",
    "    with open(output_txt, 'w') as f:\n",
    "        for pid in ids:\n",
    "            f.write(f\"{pid}\\n\")\n",
    "    print(f\"Saved IDs for {hospital} to {output_txt}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7508a53e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Hospital name</th>\n",
       "      <th>id</th>\n",
       "      <th>Basal volume, ml</th>\n",
       "      <th>FU volume, ml</th>\n",
       "      <th>Absolute vol diff, ml</th>\n",
       "      <th>Relative vol diff (FU_vol/Basal_vol)</th>\n",
       "      <th>HE vicorob</th>\n",
       "      <th>HE real</th>\n",
       "      <th>basal_img_path</th>\n",
       "      <th>basal_mask_path</th>\n",
       "      <th>fu_img_path</th>\n",
       "      <th>fu_mask_path</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Hospital Clínic</td>\n",
       "      <td>2098</td>\n",
       "      <td>5.02</td>\n",
       "      <td>4.87</td>\n",
       "      <td>-0.150</td>\n",
       "      <td>0.970</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>/media/cansu/DiskSpace/Cansu/HE_prediction_imp...</td>\n",
       "      <td>/media/cansu/DiskSpace/Cansu/HE_prediction_imp...</td>\n",
       "      <td>/media/cansu/DiskSpace/Cansu/HE_prediction_imp...</td>\n",
       "      <td>/media/cansu/DiskSpace/Cansu/HE_prediction_imp...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Hospital Clínic</td>\n",
       "      <td>233632</td>\n",
       "      <td>36.07</td>\n",
       "      <td>39.24</td>\n",
       "      <td>3.171</td>\n",
       "      <td>1.088</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>/media/cansu/DiskSpace/Cansu/HE_prediction_imp...</td>\n",
       "      <td>/media/cansu/DiskSpace/Cansu/HE_prediction_imp...</td>\n",
       "      <td>/media/cansu/DiskSpace/Cansu/HE_prediction_imp...</td>\n",
       "      <td>/media/cansu/DiskSpace/Cansu/HE_prediction_imp...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Hospital Clínic</td>\n",
       "      <td>261065</td>\n",
       "      <td>9.50</td>\n",
       "      <td>11.41</td>\n",
       "      <td>1.915</td>\n",
       "      <td>1.202</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>/media/cansu/DiskSpace/Cansu/HE_prediction_imp...</td>\n",
       "      <td>/media/cansu/DiskSpace/Cansu/HE_prediction_imp...</td>\n",
       "      <td>/media/cansu/DiskSpace/Cansu/HE_prediction_imp...</td>\n",
       "      <td>/media/cansu/DiskSpace/Cansu/HE_prediction_imp...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Hospital Clínic</td>\n",
       "      <td>397280</td>\n",
       "      <td>9.37</td>\n",
       "      <td>8.33</td>\n",
       "      <td>-1.038</td>\n",
       "      <td>0.889</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>/media/cansu/DiskSpace/Cansu/HE_prediction_imp...</td>\n",
       "      <td>/media/cansu/DiskSpace/Cansu/HE_prediction_imp...</td>\n",
       "      <td>/media/cansu/DiskSpace/Cansu/HE_prediction_imp...</td>\n",
       "      <td>/media/cansu/DiskSpace/Cansu/HE_prediction_imp...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Hospital Clínic</td>\n",
       "      <td>4141022</td>\n",
       "      <td>32.23</td>\n",
       "      <td>37.98</td>\n",
       "      <td>5.753</td>\n",
       "      <td>1.179</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>/media/cansu/DiskSpace/Cansu/HE_prediction_imp...</td>\n",
       "      <td>/media/cansu/DiskSpace/Cansu/HE_prediction_imp...</td>\n",
       "      <td>/media/cansu/DiskSpace/Cansu/HE_prediction_imp...</td>\n",
       "      <td>/media/cansu/DiskSpace/Cansu/HE_prediction_imp...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>451</th>\n",
       "      <td>Hospital Josep Trueta</td>\n",
       "      <td>pt217</td>\n",
       "      <td>0.45</td>\n",
       "      <td>22.26</td>\n",
       "      <td>21.800</td>\n",
       "      <td>49.060</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>/media/cansu/DiskSpace/Cansu/HE_prediction_imp...</td>\n",
       "      <td>/media/cansu/DiskSpace/Cansu/HE_prediction_imp...</td>\n",
       "      <td>/media/cansu/DiskSpace/Cansu/HE_prediction_imp...</td>\n",
       "      <td>/media/cansu/DiskSpace/Cansu/HE_prediction_imp...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>452</th>\n",
       "      <td>Hospital Josep Trueta</td>\n",
       "      <td>pt218</td>\n",
       "      <td>7.19</td>\n",
       "      <td>7.24</td>\n",
       "      <td>0.050</td>\n",
       "      <td>1.010</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>/media/cansu/DiskSpace/Cansu/HE_prediction_imp...</td>\n",
       "      <td>/media/cansu/DiskSpace/Cansu/HE_prediction_imp...</td>\n",
       "      <td>/media/cansu/DiskSpace/Cansu/HE_prediction_imp...</td>\n",
       "      <td>/media/cansu/DiskSpace/Cansu/HE_prediction_imp...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>453</th>\n",
       "      <td>Hospital Josep Trueta</td>\n",
       "      <td>pt219</td>\n",
       "      <td>1.09</td>\n",
       "      <td>1.39</td>\n",
       "      <td>0.300</td>\n",
       "      <td>1.280</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>/media/cansu/DiskSpace/Cansu/HE_prediction_imp...</td>\n",
       "      <td>/media/cansu/DiskSpace/Cansu/HE_prediction_imp...</td>\n",
       "      <td>/media/cansu/DiskSpace/Cansu/HE_prediction_imp...</td>\n",
       "      <td>/media/cansu/DiskSpace/Cansu/HE_prediction_imp...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>454</th>\n",
       "      <td>Hospital Josep Trueta</td>\n",
       "      <td>pt220</td>\n",
       "      <td>48.01</td>\n",
       "      <td>46.48</td>\n",
       "      <td>-1.530</td>\n",
       "      <td>0.970</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>/media/cansu/DiskSpace/Cansu/HE_prediction_imp...</td>\n",
       "      <td>/media/cansu/DiskSpace/Cansu/HE_prediction_imp...</td>\n",
       "      <td>/media/cansu/DiskSpace/Cansu/HE_prediction_imp...</td>\n",
       "      <td>/media/cansu/DiskSpace/Cansu/HE_prediction_imp...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>455</th>\n",
       "      <td>Hospital Josep Trueta</td>\n",
       "      <td>pt222</td>\n",
       "      <td>3.13</td>\n",
       "      <td>3.12</td>\n",
       "      <td>-0.010</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>/media/cansu/DiskSpace/Cansu/HE_prediction_imp...</td>\n",
       "      <td>/media/cansu/DiskSpace/Cansu/HE_prediction_imp...</td>\n",
       "      <td>/media/cansu/DiskSpace/Cansu/HE_prediction_imp...</td>\n",
       "      <td>/media/cansu/DiskSpace/Cansu/HE_prediction_imp...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>456 rows × 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             Hospital name       id  Basal volume, ml  FU volume, ml  \\\n",
       "0          Hospital Clínic     2098              5.02           4.87   \n",
       "1          Hospital Clínic   233632             36.07          39.24   \n",
       "2          Hospital Clínic   261065              9.50          11.41   \n",
       "3          Hospital Clínic   397280              9.37           8.33   \n",
       "4          Hospital Clínic  4141022             32.23          37.98   \n",
       "..                     ...      ...               ...            ...   \n",
       "451  Hospital Josep Trueta    pt217              0.45          22.26   \n",
       "452  Hospital Josep Trueta    pt218              7.19           7.24   \n",
       "453  Hospital Josep Trueta    pt219              1.09           1.39   \n",
       "454  Hospital Josep Trueta    pt220             48.01          46.48   \n",
       "455  Hospital Josep Trueta    pt222              3.13           3.12   \n",
       "\n",
       "     Absolute vol diff, ml  Relative vol diff (FU_vol/Basal_vol)  HE vicorob  \\\n",
       "0                   -0.150                                 0.970         0.0   \n",
       "1                    3.171                                 1.088         0.0   \n",
       "2                    1.915                                 1.202         0.0   \n",
       "3                   -1.038                                 0.889         0.0   \n",
       "4                    5.753                                 1.179         0.0   \n",
       "..                     ...                                   ...         ...   \n",
       "451                 21.800                                49.060         1.0   \n",
       "452                  0.050                                 1.010         0.0   \n",
       "453                  0.300                                 1.280         0.0   \n",
       "454                 -1.530                                 0.970         0.0   \n",
       "455                 -0.010                                 1.000         0.0   \n",
       "\n",
       "     HE real                                     basal_img_path  \\\n",
       "0        NaN  /media/cansu/DiskSpace/Cansu/HE_prediction_imp...   \n",
       "1        NaN  /media/cansu/DiskSpace/Cansu/HE_prediction_imp...   \n",
       "2        NaN  /media/cansu/DiskSpace/Cansu/HE_prediction_imp...   \n",
       "3        NaN  /media/cansu/DiskSpace/Cansu/HE_prediction_imp...   \n",
       "4        NaN  /media/cansu/DiskSpace/Cansu/HE_prediction_imp...   \n",
       "..       ...                                                ...   \n",
       "451      1.0  /media/cansu/DiskSpace/Cansu/HE_prediction_imp...   \n",
       "452      0.0  /media/cansu/DiskSpace/Cansu/HE_prediction_imp...   \n",
       "453      0.0  /media/cansu/DiskSpace/Cansu/HE_prediction_imp...   \n",
       "454      0.0  /media/cansu/DiskSpace/Cansu/HE_prediction_imp...   \n",
       "455      0.0  /media/cansu/DiskSpace/Cansu/HE_prediction_imp...   \n",
       "\n",
       "                                       basal_mask_path  \\\n",
       "0    /media/cansu/DiskSpace/Cansu/HE_prediction_imp...   \n",
       "1    /media/cansu/DiskSpace/Cansu/HE_prediction_imp...   \n",
       "2    /media/cansu/DiskSpace/Cansu/HE_prediction_imp...   \n",
       "3    /media/cansu/DiskSpace/Cansu/HE_prediction_imp...   \n",
       "4    /media/cansu/DiskSpace/Cansu/HE_prediction_imp...   \n",
       "..                                                 ...   \n",
       "451  /media/cansu/DiskSpace/Cansu/HE_prediction_imp...   \n",
       "452  /media/cansu/DiskSpace/Cansu/HE_prediction_imp...   \n",
       "453  /media/cansu/DiskSpace/Cansu/HE_prediction_imp...   \n",
       "454  /media/cansu/DiskSpace/Cansu/HE_prediction_imp...   \n",
       "455  /media/cansu/DiskSpace/Cansu/HE_prediction_imp...   \n",
       "\n",
       "                                           fu_img_path  \\\n",
       "0    /media/cansu/DiskSpace/Cansu/HE_prediction_imp...   \n",
       "1    /media/cansu/DiskSpace/Cansu/HE_prediction_imp...   \n",
       "2    /media/cansu/DiskSpace/Cansu/HE_prediction_imp...   \n",
       "3    /media/cansu/DiskSpace/Cansu/HE_prediction_imp...   \n",
       "4    /media/cansu/DiskSpace/Cansu/HE_prediction_imp...   \n",
       "..                                                 ...   \n",
       "451  /media/cansu/DiskSpace/Cansu/HE_prediction_imp...   \n",
       "452  /media/cansu/DiskSpace/Cansu/HE_prediction_imp...   \n",
       "453  /media/cansu/DiskSpace/Cansu/HE_prediction_imp...   \n",
       "454  /media/cansu/DiskSpace/Cansu/HE_prediction_imp...   \n",
       "455  /media/cansu/DiskSpace/Cansu/HE_prediction_imp...   \n",
       "\n",
       "                                          fu_mask_path  \n",
       "0    /media/cansu/DiskSpace/Cansu/HE_prediction_imp...  \n",
       "1    /media/cansu/DiskSpace/Cansu/HE_prediction_imp...  \n",
       "2    /media/cansu/DiskSpace/Cansu/HE_prediction_imp...  \n",
       "3    /media/cansu/DiskSpace/Cansu/HE_prediction_imp...  \n",
       "4    /media/cansu/DiskSpace/Cansu/HE_prediction_imp...  \n",
       "..                                                 ...  \n",
       "451  /media/cansu/DiskSpace/Cansu/HE_prediction_imp...  \n",
       "452  /media/cansu/DiskSpace/Cansu/HE_prediction_imp...  \n",
       "453  /media/cansu/DiskSpace/Cansu/HE_prediction_imp...  \n",
       "454  /media/cansu/DiskSpace/Cansu/HE_prediction_imp...  \n",
       "455  /media/cansu/DiskSpace/Cansu/HE_prediction_imp...  \n",
       "\n",
       "[456 rows x 12 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "# from sklearn.utils.class_weight import compute_class_weight\n",
    "# import sys; sys.path.insert(0, os.path.abspath(\"../\"))\n",
    "# from dataset import *\n",
    "# from utils import *\n",
    "# from model import *\n",
    "import torch.utils.data as data\n",
    "import random\n",
    "import argparse\n",
    "import yaml\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import SimpleITK as sitk\n",
    "from torch.utils.data import Dataset\n",
    "from pathlib import Path\n",
    "import torchvision.transforms.functional as T\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import SimpleITK as sitk\n",
    "import torch\n",
    "\n",
    "def load_nifti(path):\n",
    "    return sitk.GetArrayFromImage(sitk.ReadImage(path))\n",
    "\n",
    "def normalize_ct(volume):\n",
    "    vmin, vmax = np.percentile(volume, [1, 99])\n",
    "    volume = np.clip(volume, vmin, vmax)\n",
    "    return (volume - vmin) / (vmax - vmin + 1e-6)\n",
    "\n",
    "def has_lesion(mask_slice, threshold=2):\n",
    "    return mask_slice.sum() > threshold\n",
    "\n",
    "\n",
    "class SliceIndexBuilder:\n",
    "    def __init__(self, df, filter_lesion=True, threshold=2):\n",
    "        self.df = df\n",
    "        self.filter_lesion = filter_lesion\n",
    "        self.threshold = threshold\n",
    "        self.index = []\n",
    "\n",
    "        self._build()\n",
    "\n",
    "    def _build(self):\n",
    "        for row_idx, row in self.df.iterrows():\n",
    "            mask = load_nifti(row[\"basal_mask_path\"])\n",
    "\n",
    "            for z in range(mask.shape[0]):\n",
    "                if self.filter_lesion:\n",
    "                    if not has_lesion(mask[z], self.threshold):\n",
    "                        continue\n",
    "\n",
    "                self.index.append((row_idx, z))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.index)\n",
    "    \n",
    "\n",
    "from torch.utils.data import Dataset\n",
    "import torchvision.transforms.functional as TF\n",
    "import random\n",
    "import copy\n",
    "\n",
    "def find_normalization_parameters(image):\n",
    "    \"\"\"\n",
    "    image: numpy array with shape [1, H, W] or [D, H, W]\n",
    "    \"\"\"\n",
    "    norm_img = copy.deepcopy(image)\n",
    "    norm_parms = (\n",
    "        np.nanmin(norm_img, axis=(-3, -2, -1), keepdims=True),\n",
    "        np.nanmax(norm_img, axis=(-3, -2, -1), keepdims=True)\n",
    "    )\n",
    "    return norm_parms\n",
    "\n",
    "\n",
    "def normalize_image(image, parameters):\n",
    "    \"\"\"\n",
    "    image: numpy array\n",
    "    parameters: (min, max)\n",
    "    \"\"\"\n",
    "    return (image - parameters[0]) / (parameters[1] - parameters[0] + 1e-6)\n",
    "\n",
    "\n",
    "class HE2DSliceDataset(Dataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        df,\n",
    "        slice_index,\n",
    "        augment=False\n",
    "    ):\n",
    "        self.df = df.reset_index(drop=True)\n",
    "        self.slice_index = slice_index\n",
    "        self.augment = augment\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.slice_index)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row_idx, z = self.slice_index[idx]\n",
    "        row = self.df.iloc[row_idx]\n",
    "\n",
    "        img_3d = load_nifti(row[\"basal_img_path\"])\n",
    "        mask_3d = load_nifti(row[\"basal_mask_path\"])\n",
    "\n",
    "        slice_2d = img_3d[z]                    # [H, W]\n",
    "        mask_2d = mask_3d[z].astype(np.uint8)\n",
    "\n",
    "        # add channel dim -> [1, H, W]\n",
    "        slice_2d = slice_2d[np.newaxis, ...]\n",
    "\n",
    "        # === ORIGINAL TRAINING NORMALIZATION ===\n",
    "        norm_params = find_normalization_parameters(slice_2d)\n",
    "        slice_2d = normalize_image(slice_2d, norm_params)\n",
    "\n",
    "        img = torch.tensor(slice_2d, dtype=torch.float32)\n",
    "        mask = torch.tensor(mask_2d, dtype=torch.uint8).unsqueeze(0)\n",
    "\n",
    "        label = int(row[\"HE real\"]) if not pd.isna(row[\"HE real\"]) else -1\n",
    "        patient_id = row[\"id\"]\n",
    "\n",
    "        if self.augment:\n",
    "            if random.random() < 0.5:\n",
    "                img = TF.hflip(img)\n",
    "                mask = TF.hflip(mask)\n",
    "\n",
    "            if random.random() < 0.3:\n",
    "                angle = random.uniform(-10, 10)\n",
    "                img = TF.rotate(img, angle)\n",
    "                mask = TF.rotate(mask, angle)\n",
    "\n",
    "        return {\n",
    "            \"image\": img,\n",
    "            \"mask\": mask,\n",
    "            \"label\": torch.tensor(label),\n",
    "            \"patient_id\": patient_id,\n",
    "            \"slice_idx\": z,\n",
    "        }\n",
    "\n",
    "repo_path = os.getcwd()\n",
    "CONFIG_PATH = repo_path + '/configs'\n",
    "with open(os.path.join(CONFIG_PATH, \"/media/cansu/DiskSpace/Cansu/HE_prediction_improved/configs/config_eff_t5_repeat_1_othermain.yml\")) as file:\n",
    "    config = yaml.safe_load(file)\n",
    "\n",
    "gpu = config[\"GPU\"]\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "threshold_name = config[\"threshold_name\"]\n",
    "experiment_sample= config[\"experiment_sample\"]\n",
    "threshold = config[\"threshold_percentage\"] # 0.5\n",
    "\n",
    "# # from sklearn.model_selection import StratifiedKFold\n",
    "# # def split_labeled_dataframe(df, fold=0, n_splits=5):\n",
    "# #     skf = StratifiedKFold(\n",
    "# #         n_splits=n_splits,\n",
    "# #         shuffle=True,\n",
    "# #         random_state=42\n",
    "# #     )\n",
    "\n",
    "# #     y = df[\"HE real\"].astype(int)\n",
    "# #     splits = list(skf.split(df, y))\n",
    "\n",
    "# #     train_idx, val_idx = splits[fold]\n",
    "# #     return df.iloc[train_idx], df.iloc[val_idx]\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "md_path = repo_path + \"/data/data_with_both_basal_fu.csv\"\n",
    "df = pd.read_csv(md_path)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a11c5ead",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Lightning automatically upgraded your loaded checkpoint from v1.9.0 to v2.0.9. To apply the upgrade to your files permanently, run `python -m pytorch_lightning.utilities.upgrade_checkpoint --file checkpoints/org_da_sy_es5_hf05_repeat_2711_1_othermain/0/last_model-v1.ckpt`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using eff s\n",
      "Using focal loss\n",
      "Loading pt196: /media/cansu/DiskSpace/Cansu/HE_prediction_improved/data/RAINS_vicorob/Hospital Josep Trueta/pt196/Basal/CT_SS.nii.gz\n",
      "Loading pt022: /media/cansu/DiskSpace/Cansu/HE_prediction_improved/data/RAINS_vicorob/Hospital Josep Trueta/pt022/Basal/CT_SS.nii.gz\n",
      "Loading pt186: /media/cansu/DiskSpace/Cansu/HE_prediction_improved/data/RAINS_vicorob/Hospital Josep Trueta/pt186/Basal/CT_SS.nii.gz\n",
      "Loading pt181: /media/cansu/DiskSpace/Cansu/HE_prediction_improved/data/RAINS_vicorob/Hospital Josep Trueta/pt181/Basal/CT_SS.nii.gz\n",
      "Loading pt058: /media/cansu/DiskSpace/Cansu/HE_prediction_improved/data/RAINS_vicorob/Hospital Josep Trueta/pt058/Basal/CT_SS.nii.gz\n",
      "Loading pt205: /media/cansu/DiskSpace/Cansu/HE_prediction_improved/data/RAINS_vicorob/Hospital Josep Trueta/pt205/Basal/CT_SS.nii.gz\n",
      "Loading pt086: /media/cansu/DiskSpace/Cansu/HE_prediction_improved/data/RAINS_vicorob/Hospital Josep Trueta/pt086/Basal/CT_SS.nii.gz\n",
      "Loading pt140: /media/cansu/DiskSpace/Cansu/HE_prediction_improved/data/RAINS_vicorob/Hospital Josep Trueta/pt140/Basal/CT_SS.nii.gz\n",
      "Loading pt029: /media/cansu/DiskSpace/Cansu/HE_prediction_improved/data/RAINS_vicorob/Hospital Josep Trueta/pt029/Basal/CT_SS.nii.gz\n",
      "Loading pt155: /media/cansu/DiskSpace/Cansu/HE_prediction_improved/data/RAINS_vicorob/Hospital Josep Trueta/pt155/Basal/CT_SS.nii.gz\n",
      "Loading pt208: /media/cansu/DiskSpace/Cansu/HE_prediction_improved/data/RAINS_vicorob/Hospital Josep Trueta/pt208/Basal/CT_SS.nii.gz\n",
      "Loading pt098: /media/cansu/DiskSpace/Cansu/HE_prediction_improved/data/RAINS_vicorob/Hospital Josep Trueta/pt098/Basal/CT_SS.nii.gz\n",
      "Loading pt126: /media/cansu/DiskSpace/Cansu/HE_prediction_improved/data/RAINS_vicorob/Hospital Josep Trueta/pt126/Basal/CT_SS.nii.gz\n",
      "Loading pt191: /media/cansu/DiskSpace/Cansu/HE_prediction_improved/data/RAINS_vicorob/Hospital Josep Trueta/pt191/Basal/CT_SS.nii.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inference Hospital_josep_trueta_gt: 100%|██████████| 355/355 [01:43<00:00,  3.44it/s]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "import SimpleITK as sitk\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm import tqdm\n",
    "\n",
    "def load_nifti(path):\n",
    "    assert path.endswith((\".nii\", \".nii.gz\")), path\n",
    "    assert os.path.exists(path), path\n",
    "    img = sitk.ReadImage(path, sitk.sitkFloat32)\n",
    "    img = sitk.GetArrayFromImage(img)\n",
    "    return img\n",
    "\n",
    "def find_normalization_parameters(image):\n",
    "    \"\"\"\n",
    "    image: numpy array with shape [1, H, W] or [D, H, W]\n",
    "    \"\"\"\n",
    "    norm_img = copy.deepcopy(image)\n",
    "    norm_parms = (\n",
    "        np.nanmin(norm_img, axis=(-3, -2, -1), keepdims=True),\n",
    "        np.nanmax(norm_img, axis=(-3, -2, -1), keepdims=True)\n",
    "    )\n",
    "    return norm_parms\n",
    "\n",
    "def normalize_image(image, parameters):\n",
    "    \"\"\"\n",
    "    image: numpy array\n",
    "    parameters: (min, max)\n",
    "    \"\"\"\n",
    "    return (image - parameters[0]) / (parameters[1] - parameters[0] + 1e-6)\n",
    "\n",
    "\n",
    "class HE2DInferenceDataset(Dataset):\n",
    "    def __init__(self, patient_ids, metadata_df):\n",
    "        self.patient_ids = patient_ids\n",
    "        self.df = metadata_df.set_index(\"id\")\n",
    "        self.samples = []\n",
    "        self._build_index()\n",
    "\n",
    "    def _build_index(self):\n",
    "        for pid in self.patient_ids:\n",
    "            try:\n",
    "                row = self.df.loc[pid]\n",
    "                path = row[\"basal_img_path\"]\n",
    "                print(f\"Loading {pid}: {path}\")\n",
    "\n",
    "                img = load_nifti(path)\n",
    "\n",
    "                for z in range(img.shape[0]):\n",
    "                    self.samples.append((pid, z))\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"❌ Failed for pid={pid}\")\n",
    "                raise e\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        pid, z = self.samples[idx]\n",
    "        row = self.df.loc[pid]\n",
    "\n",
    "        img = load_nifti(row[\"basal_img_path\"])\n",
    "        pseudo_label = row[\"HE vicorob\"]\n",
    "        gt_label = row[\"HE real\"] if not pd.isna(row[\"HE real\"]) else -1\n",
    "\n",
    "        norm_params = find_normalization_parameters(img[z][np.newaxis, ...])\n",
    "        img = normalize_image(img[z][np.newaxis, ...], norm_params)\n",
    "        img = torch.tensor(img, dtype=torch.float32)\n",
    "\n",
    "        # concat 2 images with the mask image together so that there will be 3 channels\n",
    "        mask = load_nifti(row[\"basal_mask_path\"])\n",
    "        mask_slice = mask[z].astype(np.uint8)\n",
    "        mask = torch.from_numpy(mask_slice[np.newaxis, ...]).long()\n",
    "        # # concat image + image + mask\n",
    "        img = torch.cat([img, img], dim=0)\n",
    "        img = torch.cat([img, mask], dim=0)\n",
    "\n",
    "        return img, pid, z, pseudo_label, gt_label\n",
    "\n",
    "checkpoint_path = \"/media/cansu/DiskSpace/Cansu/HE_prediction_improved/checkpoints/org_da_sy_es5_hf05_repeat_2711_1_othermain/0/last_model-v1.ckpt\"\n",
    "\n",
    "from model import ImageClassifier3\n",
    "model = ImageClassifier3.load_from_checkpoint(checkpoint_path)\n",
    "model.eval().cuda(device=0)\n",
    "\n",
    "def read_ids(txt_path):\n",
    "    with open(txt_path, \"r\") as f:\n",
    "        return [line.strip() for line in f if line.strip()]\n",
    "\n",
    "txt_files = {\n",
    "    # \"Hospital_josep_trueta_pseudo\": \"/media/cansu/DiskSpace/Cansu/HE_prediction_improved/data/splits/Hospital_Josep_Trueta_pseudo_ids.txt\",\n",
    "    \"Hospital_josep_trueta_gt\": \"/media/cansu/DiskSpace/Cansu/HE_prediction_improved/data/splits/Hospital_Josep_Trueta_gt_ids.txt\", # test for fold 0. \n",
    "    # \"Hospital_clinic\": \"/media/cansu/DiskSpace/Cansu/HE_prediction_improved/data/splits/Hospital_Clínic_ids.txt\",\n",
    "    # \"Hospital_sant_pau\": \"/media/cansu/DiskSpace/Cansu/HE_prediction_improved/data/splits/Hospital_Sant_Pau_ids.txt\",\n",
    "}\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "md_path = \"/media/cansu/DiskSpace/Cansu/HE_prediction_improved/data/data_with_both_basal_fu.csv\"\n",
    "metadata_df = pd.read_csv(md_path)\n",
    "\n",
    "results = []\n",
    "\n",
    "for split_name, txt_path in txt_files.items():\n",
    "    patient_ids = read_ids(txt_path)\n",
    "    ds = HE2DInferenceDataset(patient_ids, metadata_df)\n",
    "    loader = DataLoader(ds, batch_size=4, num_workers=8, shuffle=False)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for img, pid, z, pseudo_label, gt_label in tqdm(loader, desc=f\"Inference {split_name}\"):\n",
    "            pseudo_label = np.array(pseudo_label).astype(int)\n",
    "            gt_label = np.array(gt_label).astype(int)\n",
    "            img = img.cuda(device=0)\n",
    "            logits = model(img)\n",
    "            probs = torch.sigmoid(logits).cpu().numpy().squeeze()\n",
    "            prediction = (probs >= 0.5).astype(int)\n",
    "\n",
    "            for i in range(len(pid)):\n",
    "                results.append({\n",
    "                    \"split\": split_name,\n",
    "                    \"patient_id\": pid[i],\n",
    "                    \"slice_idx\": int(z[i]),\n",
    "                    \"prob_he\": float(probs[i]),\n",
    "                    \"pseudo_label\": pseudo_label[i],\n",
    "                    \"gt_label\": gt_label[i],\n",
    "                    \"prediction\": prediction[i],\n",
    "                })\n",
    "\n",
    "res_df = pd.DataFrame(results)\n",
    "\n",
    "patient_df = (\n",
    "    res_df\n",
    "    .groupby([\"split\", \"patient_id\"])\n",
    "    .agg(\n",
    "        mean_prob=(\"prob_he\", \"mean\"),\n",
    "        max_prob=(\"prob_he\", \"max\"),\n",
    "        n_slices=(\"prob_he\", \"count\"),\n",
    "        pseudo_label=(\"pseudo_label\", \"first\"),\n",
    "        gt_label=(\"gt_label\", \"first\"),\n",
    "        prediction=(\"prediction\", lambda x: int(np.mean(x) >= 0.5)),\n",
    "        )\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "res_df.to_csv(\"/media/cansu/DiskSpace/Cansu/HE_prediction_improved/data/test_on_others/slice_level_predictions.csv\", index=False)\n",
    "patient_df.to_csv(\"/media/cansu/DiskSpace/Cansu/HE_prediction_improved/data/test_on_others/patient_level_predictions.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d45b689a",
   "metadata": {},
   "source": [
    "Todos:\n",
    "1. get the prediction results using the old code -- dataset and dataloader. for the test set for fold 0. \n",
    "2. get the pred results for the same data but for using new dataset and dataloader."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b4ef75fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using eff s\n",
      "Using focal loss\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 52/52 [00:00<00:00, 49909.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of slices after filtering in test  1264\n",
      "number of slices in the test set:  1264\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "import sys; sys.path.insert(0, os.path.abspath(\"../\"))\n",
    "from dataset import *\n",
    "from utils import *\n",
    "from model import *\n",
    "from model import ImageClassifier2\n",
    "import torch.utils.data as data\n",
    "import random\n",
    "import argparse\n",
    "import yaml\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "from tqdm import tqdm\n",
    "\n",
    "# chcekpoint for the best da_sy(5) model\n",
    "checkpoint_path = \"/media/cansu/DiskSpace/Cansu/HE_prediction_improved/checkpoints/org_da_sy_es5_hf05_repeat_2711_1_othermain/0/last_model-v1.ckpt\"\n",
    "\n",
    "model = ImageClassifier3.load_from_checkpoint(checkpoint_path)\n",
    "model.eval().cuda(device=0)\n",
    "\n",
    "unseen_neg_samples = pd.read_csv(\"/media/cansu/DiskSpace/Cansu/HE_prediction_improved/data/not_sampled_negative_52_cases.csv\")\n",
    "\n",
    "X_train =[]\n",
    "y_train = []\n",
    "X_val = []\n",
    "X_test = []\n",
    "y_val = []\n",
    "y_test = []\n",
    "\n",
    "md_path = \"/media/cansu/DiskSpace/Cansu/HE_prediction_improved/data/metadata.csv\"\n",
    "\n",
    "for i in tqdm(range(len(unseen_neg_samples))):\n",
    "    # add the index numbers of the unseen negative samples to the X_test list\n",
    "    X_test.append(unseen_neg_samples[\"index\"].tolist()[i])\n",
    "    y_test.append(unseen_neg_samples[\"label\"].tolist()[i])\n",
    "\n",
    "indexes = X_train, X_val, X_test, y_train, y_val, y_test\n",
    "\n",
    "dm = HEPredDataModule(split_indexes=indexes, \n",
    "                        filter_slices=True,\n",
    "                        mask=True, \n",
    "                        batch_size=1, \n",
    "                        num_workers=8, \n",
    "                        use_2d=True, \n",
    "                        return_type='image',\n",
    "                        under_sampling=False,\n",
    "                        over_sampling=False,\n",
    "                        threshold=0.5,\n",
    "                        md_path=md_path,\n",
    "                        basal_fu=False,\n",
    "                        roi = False, problem = \"prediction\",\n",
    "                        image_size = 512,\n",
    "                        roi_size=512, lesion=False,\n",
    "                        test_type = \"t5\",\n",
    "                        apply_hflip = False,\n",
    "                        apply_affine = False,\n",
    "                        apply_gaussian_blur= False,\n",
    "                        affine_degree=10,\n",
    "                        affine_translate=0,\n",
    "                        affine_scale=1.0,\n",
    "                        affine_shear=0, \n",
    "                        hflip_p = 0.5, affine_p = 0.5,)\n",
    "dm.setup()\n",
    "\n",
    "# Read the test dataset from the datamodule\n",
    "\n",
    "test_loader = dm.test_dataloader()\n",
    "\n",
    "img_list = []\n",
    "label_list = []\n",
    "id_list = []\n",
    "snum_list = []\n",
    "\n",
    "for batch in test_loader:\n",
    "    image, label, id, snum = batch\n",
    "    img_list.append(image)\n",
    "    label_list.append(label)\n",
    "    id_list.append(id)\n",
    "    snum_list.append(snum)\n",
    "\n",
    "probabilities = []\n",
    "for i in range(len(id_list)):\n",
    "    with torch.no_grad():\n",
    "        y_hat = model(img_list[i].cuda(device=0))\n",
    "        probabilities.append(y_hat.squeeze().cpu().numpy().tolist())\n",
    "\n",
    "#make a dataframe with the id, snum and probabilities\n",
    "df = pd.DataFrame(list(zip(id_list, snum_list, probabilities)), columns = [\"id\", \"snum\", \"probabilities\"])\n",
    "# change the id column to integer\n",
    "df[\"id\"] = df[\"id\"].astype(int)\n",
    "\n",
    "# for each patient id take the probablity average of the slices\n",
    "df_patient_averages = pd.DataFrame(columns = [\"id\", \"average_probabilities\"])\n",
    "\n",
    "id_dict = {}\n",
    "for i in range(len(df)):\n",
    "    current_id = df[\"id\"][i]\n",
    "    probability = df[\"probabilities\"][i]\n",
    "    if current_id in id_dict:\n",
    "        id_dict[current_id].append(probability)\n",
    "    else:\n",
    "        id_dict[current_id] = [probability]\n",
    "\n",
    "# for each id, take the average of the probablities\n",
    "for id, probablities in id_dict.items():\n",
    "    average = sum(probablities) / len(probablities)\n",
    "    # DataFrame.append is removed in recent pandas versions. Use .loc to add a new row instead.\n",
    "    df_patient_averages.loc[len(df_patient_averages)] = {\"id\": id, \"average_probabilities\": average}\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4cf54345",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>average_probabilities</th>\n",
       "      <th>predicted_labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>118</td>\n",
       "      <td>-2.455591</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>142</td>\n",
       "      <td>-0.822859</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>72</td>\n",
       "      <td>-1.060153</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>135</td>\n",
       "      <td>-1.050091</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>80</td>\n",
       "      <td>0.556763</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>167</td>\n",
       "      <td>-1.479259</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>132</td>\n",
       "      <td>0.509654</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>78</td>\n",
       "      <td>-1.902653</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>62</td>\n",
       "      <td>-1.396133</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>63</td>\n",
       "      <td>-2.072023</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>218</td>\n",
       "      <td>-1.309894</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>119</td>\n",
       "      <td>-2.466853</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>14</td>\n",
       "      <td>0.811827</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>124</td>\n",
       "      <td>-3.276939</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>202</td>\n",
       "      <td>-2.327263</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>9</td>\n",
       "      <td>0.761009</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>41</td>\n",
       "      <td>-2.512073</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>194</td>\n",
       "      <td>-2.968612</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>19</td>\n",
       "      <td>-1.889841</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>16</td>\n",
       "      <td>0.167380</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>67</td>\n",
       "      <td>-2.939813</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>32</td>\n",
       "      <td>-0.733563</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>59</td>\n",
       "      <td>-1.537731</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>131</td>\n",
       "      <td>-1.819160</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>209</td>\n",
       "      <td>-3.099141</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>104</td>\n",
       "      <td>-0.205079</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>3</td>\n",
       "      <td>-3.072269</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>84</td>\n",
       "      <td>-2.031618</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>129</td>\n",
       "      <td>-0.382813</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>83</td>\n",
       "      <td>0.944827</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>203</td>\n",
       "      <td>-2.539924</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>39</td>\n",
       "      <td>-1.569563</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>159</td>\n",
       "      <td>-2.485797</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>133</td>\n",
       "      <td>-2.160223</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>122</td>\n",
       "      <td>-3.097759</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>13</td>\n",
       "      <td>-1.716560</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>168</td>\n",
       "      <td>-1.767050</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>212</td>\n",
       "      <td>-1.748688</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>175</td>\n",
       "      <td>-5.074878</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>165</td>\n",
       "      <td>-1.672188</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>128</td>\n",
       "      <td>-2.862534</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>12</td>\n",
       "      <td>-2.280876</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>111</td>\n",
       "      <td>-1.800418</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>50</td>\n",
       "      <td>-3.245631</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>24</td>\n",
       "      <td>-1.300395</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>23</td>\n",
       "      <td>-2.084070</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>136</td>\n",
       "      <td>-2.827987</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>73</td>\n",
       "      <td>-0.345124</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>99</td>\n",
       "      <td>-1.469919</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>51</td>\n",
       "      <td>-0.610718</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>15</td>\n",
       "      <td>-0.759349</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>102</td>\n",
       "      <td>-0.546124</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     id  average_probabilities  predicted_labels\n",
       "0   118              -2.455591                 0\n",
       "1   142              -0.822859                 0\n",
       "2    72              -1.060153                 0\n",
       "3   135              -1.050091                 0\n",
       "4    80               0.556763                 1\n",
       "5   167              -1.479259                 0\n",
       "6   132               0.509654                 1\n",
       "7    78              -1.902653                 0\n",
       "8    62              -1.396133                 0\n",
       "9    63              -2.072023                 0\n",
       "10  218              -1.309894                 0\n",
       "11  119              -2.466853                 0\n",
       "12   14               0.811827                 1\n",
       "13  124              -3.276939                 0\n",
       "14  202              -2.327263                 0\n",
       "15    9               0.761009                 1\n",
       "16   41              -2.512073                 0\n",
       "17  194              -2.968612                 0\n",
       "18   19              -1.889841                 0\n",
       "19   16               0.167380                 0\n",
       "20   67              -2.939813                 0\n",
       "21   32              -0.733563                 0\n",
       "22   59              -1.537731                 0\n",
       "23  131              -1.819160                 0\n",
       "24  209              -3.099141                 0\n",
       "25  104              -0.205079                 0\n",
       "26    3              -3.072269                 0\n",
       "27   84              -2.031618                 0\n",
       "28  129              -0.382813                 0\n",
       "29   83               0.944827                 1\n",
       "30  203              -2.539924                 0\n",
       "31   39              -1.569563                 0\n",
       "32  159              -2.485797                 0\n",
       "33  133              -2.160223                 0\n",
       "34  122              -3.097759                 0\n",
       "35   13              -1.716560                 0\n",
       "36  168              -1.767050                 0\n",
       "37  212              -1.748688                 0\n",
       "38  175              -5.074878                 0\n",
       "39  165              -1.672188                 0\n",
       "40  128              -2.862534                 0\n",
       "41   12              -2.280876                 0\n",
       "42  111              -1.800418                 0\n",
       "43   50              -3.245631                 0\n",
       "44   24              -1.300395                 0\n",
       "45   23              -2.084070                 0\n",
       "46  136              -2.827987                 0\n",
       "47   73              -0.345124                 0\n",
       "48   99              -1.469919                 0\n",
       "49   51              -0.610718                 0\n",
       "50   15              -0.759349                 0\n",
       "51  102              -0.546124                 0"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# add another column as predicted labels where if the probablity is greater than 0.5, label is 1, else 0\n",
    "df_patient_averages[\"predicted_labels\"] = df_patient_averages[\"average_probabilities\"].apply(lambda x: 1 if x > 0.5 else 0)\n",
    "df_patient_averages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b01057f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>average_probabilities</th>\n",
       "      <th>predicted_labels</th>\n",
       "      <th>actual_labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>118</td>\n",
       "      <td>-2.455591</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>142</td>\n",
       "      <td>-0.822859</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>72</td>\n",
       "      <td>-1.060153</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>135</td>\n",
       "      <td>-1.050091</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>80</td>\n",
       "      <td>0.556763</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>167</td>\n",
       "      <td>-1.479259</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>132</td>\n",
       "      <td>0.509654</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>78</td>\n",
       "      <td>-1.902653</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>62</td>\n",
       "      <td>-1.396133</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>63</td>\n",
       "      <td>-2.072023</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>218</td>\n",
       "      <td>-1.309894</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>119</td>\n",
       "      <td>-2.466853</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>14</td>\n",
       "      <td>0.811827</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>124</td>\n",
       "      <td>-3.276939</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>202</td>\n",
       "      <td>-2.327263</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>9</td>\n",
       "      <td>0.761009</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>41</td>\n",
       "      <td>-2.512073</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>194</td>\n",
       "      <td>-2.968612</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>19</td>\n",
       "      <td>-1.889841</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>16</td>\n",
       "      <td>0.167380</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>67</td>\n",
       "      <td>-2.939813</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>32</td>\n",
       "      <td>-0.733563</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>59</td>\n",
       "      <td>-1.537731</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>131</td>\n",
       "      <td>-1.819160</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>209</td>\n",
       "      <td>-3.099141</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>104</td>\n",
       "      <td>-0.205079</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>3</td>\n",
       "      <td>-3.072269</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>84</td>\n",
       "      <td>-2.031618</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>129</td>\n",
       "      <td>-0.382813</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>83</td>\n",
       "      <td>0.944827</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>203</td>\n",
       "      <td>-2.539924</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>39</td>\n",
       "      <td>-1.569563</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>159</td>\n",
       "      <td>-2.485797</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>133</td>\n",
       "      <td>-2.160223</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>122</td>\n",
       "      <td>-3.097759</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>13</td>\n",
       "      <td>-1.716560</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>168</td>\n",
       "      <td>-1.767050</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>212</td>\n",
       "      <td>-1.748688</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>175</td>\n",
       "      <td>-5.074878</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>165</td>\n",
       "      <td>-1.672188</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>128</td>\n",
       "      <td>-2.862534</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>12</td>\n",
       "      <td>-2.280876</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>111</td>\n",
       "      <td>-1.800418</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>50</td>\n",
       "      <td>-3.245631</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>24</td>\n",
       "      <td>-1.300395</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>23</td>\n",
       "      <td>-2.084070</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>136</td>\n",
       "      <td>-2.827987</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>73</td>\n",
       "      <td>-0.345124</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>99</td>\n",
       "      <td>-1.469919</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>51</td>\n",
       "      <td>-0.610718</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>15</td>\n",
       "      <td>-0.759349</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>102</td>\n",
       "      <td>-0.546124</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     id  average_probabilities  predicted_labels  actual_labels\n",
       "0   118              -2.455591                 0              0\n",
       "1   142              -0.822859                 0              0\n",
       "2    72              -1.060153                 0              0\n",
       "3   135              -1.050091                 0              0\n",
       "4    80               0.556763                 1              0\n",
       "5   167              -1.479259                 0              0\n",
       "6   132               0.509654                 1              0\n",
       "7    78              -1.902653                 0              0\n",
       "8    62              -1.396133                 0              0\n",
       "9    63              -2.072023                 0              0\n",
       "10  218              -1.309894                 0              0\n",
       "11  119              -2.466853                 0              0\n",
       "12   14               0.811827                 1              0\n",
       "13  124              -3.276939                 0              0\n",
       "14  202              -2.327263                 0              0\n",
       "15    9               0.761009                 1              0\n",
       "16   41              -2.512073                 0              0\n",
       "17  194              -2.968612                 0              0\n",
       "18   19              -1.889841                 0              0\n",
       "19   16               0.167380                 0              0\n",
       "20   67              -2.939813                 0              0\n",
       "21   32              -0.733563                 0              0\n",
       "22   59              -1.537731                 0              0\n",
       "23  131              -1.819160                 0              0\n",
       "24  209              -3.099141                 0              0\n",
       "25  104              -0.205079                 0              0\n",
       "26    3              -3.072269                 0              0\n",
       "27   84              -2.031618                 0              0\n",
       "28  129              -0.382813                 0              0\n",
       "29   83               0.944827                 1              0\n",
       "30  203              -2.539924                 0              0\n",
       "31   39              -1.569563                 0              0\n",
       "32  159              -2.485797                 0              0\n",
       "33  133              -2.160223                 0              0\n",
       "34  122              -3.097759                 0              0\n",
       "35   13              -1.716560                 0              0\n",
       "36  168              -1.767050                 0              0\n",
       "37  212              -1.748688                 0              0\n",
       "38  175              -5.074878                 0              0\n",
       "39  165              -1.672188                 0              0\n",
       "40  128              -2.862534                 0              0\n",
       "41   12              -2.280876                 0              0\n",
       "42  111              -1.800418                 0              0\n",
       "43   50              -3.245631                 0              0\n",
       "44   24              -1.300395                 0              0\n",
       "45   23              -2.084070                 0              0\n",
       "46  136              -2.827987                 0              0\n",
       "47   73              -0.345124                 0              0\n",
       "48   99              -1.469919                 0              0\n",
       "49   51              -0.610718                 0              0\n",
       "50   15              -0.759349                 0              0\n",
       "51  102              -0.546124                 0              0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# add the actual labels to the df_patient_averages where all of them are 0's\n",
    "df_patient_averages[\"actual_labels\"] = 0\n",
    "df_patient_averages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "99f2c8a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.9038461538461539\n",
      "Sensitivity:  nan\n",
      "Specificity:  0.9038461538461539\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1941715/1794207831.py:6: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  sensitivity = true_positives.sum() / (true_positives.sum() + false_negatives.sum())\n"
     ]
    }
   ],
   "source": [
    "def compute_sensitivity_specificity(preds, labels):\n",
    "    true_positives = (preds == 1) & (labels == 1)\n",
    "    true_negatives = (preds == 0) & (labels == 0)\n",
    "    false_positives = (preds == 1) & (labels == 0)\n",
    "    false_negatives = (preds == 0) & (labels == 1)\n",
    "    sensitivity = true_positives.sum() / (true_positives.sum() + false_negatives.sum())\n",
    "    specificity = true_negatives.sum() / (true_negatives.sum() + false_positives.sum())\n",
    "    return sensitivity, specificity\n",
    "\n",
    "# calculate the accuracy\n",
    "from sklearn.metrics import accuracy_score\n",
    "accuracy = accuracy_score(df_patient_averages[\"actual_labels\"], df_patient_averages[\"predicted_labels\"])\n",
    "\n",
    "# sensitivity and specificity\n",
    "sensitivity, specificity = compute_sensitivity_specificity(df_patient_averages[\"predicted_labels\"], df_patient_averages[\"actual_labels\"])\n",
    "\n",
    "print(\"Accuracy: \", accuracy)\n",
    "print(\"Sensitivity: \", sensitivity)\n",
    "print(\"Specificity: \", specificity)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ed691f1",
   "metadata": {},
   "source": [
    "# create new csv files for the ones I will test for (similar way to the negative samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dc75d8d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Hospital name</th>\n",
       "      <th>id</th>\n",
       "      <th>Basal volume, ml</th>\n",
       "      <th>FU volume, ml</th>\n",
       "      <th>Absolute vol diff, ml</th>\n",
       "      <th>Relative vol diff (FU_vol/Basal_vol)</th>\n",
       "      <th>HE vicorob</th>\n",
       "      <th>HE real</th>\n",
       "      <th>basal_img_path</th>\n",
       "      <th>basal_mask_path</th>\n",
       "      <th>fu_img_path</th>\n",
       "      <th>fu_mask_path</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Hospital Clínic</td>\n",
       "      <td>2098</td>\n",
       "      <td>5.02</td>\n",
       "      <td>4.87</td>\n",
       "      <td>-0.150</td>\n",
       "      <td>0.970</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>/media/cansu/DiskSpace/Cansu/HE_prediction_imp...</td>\n",
       "      <td>/media/cansu/DiskSpace/Cansu/HE_prediction_imp...</td>\n",
       "      <td>/media/cansu/DiskSpace/Cansu/HE_prediction_imp...</td>\n",
       "      <td>/media/cansu/DiskSpace/Cansu/HE_prediction_imp...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Hospital Clínic</td>\n",
       "      <td>233632</td>\n",
       "      <td>36.07</td>\n",
       "      <td>39.24</td>\n",
       "      <td>3.171</td>\n",
       "      <td>1.088</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>/media/cansu/DiskSpace/Cansu/HE_prediction_imp...</td>\n",
       "      <td>/media/cansu/DiskSpace/Cansu/HE_prediction_imp...</td>\n",
       "      <td>/media/cansu/DiskSpace/Cansu/HE_prediction_imp...</td>\n",
       "      <td>/media/cansu/DiskSpace/Cansu/HE_prediction_imp...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Hospital Clínic</td>\n",
       "      <td>261065</td>\n",
       "      <td>9.50</td>\n",
       "      <td>11.41</td>\n",
       "      <td>1.915</td>\n",
       "      <td>1.202</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>/media/cansu/DiskSpace/Cansu/HE_prediction_imp...</td>\n",
       "      <td>/media/cansu/DiskSpace/Cansu/HE_prediction_imp...</td>\n",
       "      <td>/media/cansu/DiskSpace/Cansu/HE_prediction_imp...</td>\n",
       "      <td>/media/cansu/DiskSpace/Cansu/HE_prediction_imp...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Hospital Clínic</td>\n",
       "      <td>397280</td>\n",
       "      <td>9.37</td>\n",
       "      <td>8.33</td>\n",
       "      <td>-1.038</td>\n",
       "      <td>0.889</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>/media/cansu/DiskSpace/Cansu/HE_prediction_imp...</td>\n",
       "      <td>/media/cansu/DiskSpace/Cansu/HE_prediction_imp...</td>\n",
       "      <td>/media/cansu/DiskSpace/Cansu/HE_prediction_imp...</td>\n",
       "      <td>/media/cansu/DiskSpace/Cansu/HE_prediction_imp...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Hospital Clínic</td>\n",
       "      <td>4141022</td>\n",
       "      <td>32.23</td>\n",
       "      <td>37.98</td>\n",
       "      <td>5.753</td>\n",
       "      <td>1.179</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>/media/cansu/DiskSpace/Cansu/HE_prediction_imp...</td>\n",
       "      <td>/media/cansu/DiskSpace/Cansu/HE_prediction_imp...</td>\n",
       "      <td>/media/cansu/DiskSpace/Cansu/HE_prediction_imp...</td>\n",
       "      <td>/media/cansu/DiskSpace/Cansu/HE_prediction_imp...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>451</th>\n",
       "      <td>Hospital Josep Trueta</td>\n",
       "      <td>pt217</td>\n",
       "      <td>0.45</td>\n",
       "      <td>22.26</td>\n",
       "      <td>21.800</td>\n",
       "      <td>49.060</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>/media/cansu/DiskSpace/Cansu/HE_prediction_imp...</td>\n",
       "      <td>/media/cansu/DiskSpace/Cansu/HE_prediction_imp...</td>\n",
       "      <td>/media/cansu/DiskSpace/Cansu/HE_prediction_imp...</td>\n",
       "      <td>/media/cansu/DiskSpace/Cansu/HE_prediction_imp...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>452</th>\n",
       "      <td>Hospital Josep Trueta</td>\n",
       "      <td>pt218</td>\n",
       "      <td>7.19</td>\n",
       "      <td>7.24</td>\n",
       "      <td>0.050</td>\n",
       "      <td>1.010</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>/media/cansu/DiskSpace/Cansu/HE_prediction_imp...</td>\n",
       "      <td>/media/cansu/DiskSpace/Cansu/HE_prediction_imp...</td>\n",
       "      <td>/media/cansu/DiskSpace/Cansu/HE_prediction_imp...</td>\n",
       "      <td>/media/cansu/DiskSpace/Cansu/HE_prediction_imp...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>453</th>\n",
       "      <td>Hospital Josep Trueta</td>\n",
       "      <td>pt219</td>\n",
       "      <td>1.09</td>\n",
       "      <td>1.39</td>\n",
       "      <td>0.300</td>\n",
       "      <td>1.280</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>/media/cansu/DiskSpace/Cansu/HE_prediction_imp...</td>\n",
       "      <td>/media/cansu/DiskSpace/Cansu/HE_prediction_imp...</td>\n",
       "      <td>/media/cansu/DiskSpace/Cansu/HE_prediction_imp...</td>\n",
       "      <td>/media/cansu/DiskSpace/Cansu/HE_prediction_imp...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>454</th>\n",
       "      <td>Hospital Josep Trueta</td>\n",
       "      <td>pt220</td>\n",
       "      <td>48.01</td>\n",
       "      <td>46.48</td>\n",
       "      <td>-1.530</td>\n",
       "      <td>0.970</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>/media/cansu/DiskSpace/Cansu/HE_prediction_imp...</td>\n",
       "      <td>/media/cansu/DiskSpace/Cansu/HE_prediction_imp...</td>\n",
       "      <td>/media/cansu/DiskSpace/Cansu/HE_prediction_imp...</td>\n",
       "      <td>/media/cansu/DiskSpace/Cansu/HE_prediction_imp...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>455</th>\n",
       "      <td>Hospital Josep Trueta</td>\n",
       "      <td>pt222</td>\n",
       "      <td>3.13</td>\n",
       "      <td>3.12</td>\n",
       "      <td>-0.010</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>/media/cansu/DiskSpace/Cansu/HE_prediction_imp...</td>\n",
       "      <td>/media/cansu/DiskSpace/Cansu/HE_prediction_imp...</td>\n",
       "      <td>/media/cansu/DiskSpace/Cansu/HE_prediction_imp...</td>\n",
       "      <td>/media/cansu/DiskSpace/Cansu/HE_prediction_imp...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>456 rows × 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             Hospital name       id  Basal volume, ml  FU volume, ml  \\\n",
       "0          Hospital Clínic     2098              5.02           4.87   \n",
       "1          Hospital Clínic   233632             36.07          39.24   \n",
       "2          Hospital Clínic   261065              9.50          11.41   \n",
       "3          Hospital Clínic   397280              9.37           8.33   \n",
       "4          Hospital Clínic  4141022             32.23          37.98   \n",
       "..                     ...      ...               ...            ...   \n",
       "451  Hospital Josep Trueta    pt217              0.45          22.26   \n",
       "452  Hospital Josep Trueta    pt218              7.19           7.24   \n",
       "453  Hospital Josep Trueta    pt219              1.09           1.39   \n",
       "454  Hospital Josep Trueta    pt220             48.01          46.48   \n",
       "455  Hospital Josep Trueta    pt222              3.13           3.12   \n",
       "\n",
       "     Absolute vol diff, ml  Relative vol diff (FU_vol/Basal_vol)  HE vicorob  \\\n",
       "0                   -0.150                                 0.970         0.0   \n",
       "1                    3.171                                 1.088         0.0   \n",
       "2                    1.915                                 1.202         0.0   \n",
       "3                   -1.038                                 0.889         0.0   \n",
       "4                    5.753                                 1.179         0.0   \n",
       "..                     ...                                   ...         ...   \n",
       "451                 21.800                                49.060         1.0   \n",
       "452                  0.050                                 1.010         0.0   \n",
       "453                  0.300                                 1.280         0.0   \n",
       "454                 -1.530                                 0.970         0.0   \n",
       "455                 -0.010                                 1.000         0.0   \n",
       "\n",
       "     HE real                                     basal_img_path  \\\n",
       "0        NaN  /media/cansu/DiskSpace/Cansu/HE_prediction_imp...   \n",
       "1        NaN  /media/cansu/DiskSpace/Cansu/HE_prediction_imp...   \n",
       "2        NaN  /media/cansu/DiskSpace/Cansu/HE_prediction_imp...   \n",
       "3        NaN  /media/cansu/DiskSpace/Cansu/HE_prediction_imp...   \n",
       "4        NaN  /media/cansu/DiskSpace/Cansu/HE_prediction_imp...   \n",
       "..       ...                                                ...   \n",
       "451      1.0  /media/cansu/DiskSpace/Cansu/HE_prediction_imp...   \n",
       "452      0.0  /media/cansu/DiskSpace/Cansu/HE_prediction_imp...   \n",
       "453      0.0  /media/cansu/DiskSpace/Cansu/HE_prediction_imp...   \n",
       "454      0.0  /media/cansu/DiskSpace/Cansu/HE_prediction_imp...   \n",
       "455      0.0  /media/cansu/DiskSpace/Cansu/HE_prediction_imp...   \n",
       "\n",
       "                                       basal_mask_path  \\\n",
       "0    /media/cansu/DiskSpace/Cansu/HE_prediction_imp...   \n",
       "1    /media/cansu/DiskSpace/Cansu/HE_prediction_imp...   \n",
       "2    /media/cansu/DiskSpace/Cansu/HE_prediction_imp...   \n",
       "3    /media/cansu/DiskSpace/Cansu/HE_prediction_imp...   \n",
       "4    /media/cansu/DiskSpace/Cansu/HE_prediction_imp...   \n",
       "..                                                 ...   \n",
       "451  /media/cansu/DiskSpace/Cansu/HE_prediction_imp...   \n",
       "452  /media/cansu/DiskSpace/Cansu/HE_prediction_imp...   \n",
       "453  /media/cansu/DiskSpace/Cansu/HE_prediction_imp...   \n",
       "454  /media/cansu/DiskSpace/Cansu/HE_prediction_imp...   \n",
       "455  /media/cansu/DiskSpace/Cansu/HE_prediction_imp...   \n",
       "\n",
       "                                           fu_img_path  \\\n",
       "0    /media/cansu/DiskSpace/Cansu/HE_prediction_imp...   \n",
       "1    /media/cansu/DiskSpace/Cansu/HE_prediction_imp...   \n",
       "2    /media/cansu/DiskSpace/Cansu/HE_prediction_imp...   \n",
       "3    /media/cansu/DiskSpace/Cansu/HE_prediction_imp...   \n",
       "4    /media/cansu/DiskSpace/Cansu/HE_prediction_imp...   \n",
       "..                                                 ...   \n",
       "451  /media/cansu/DiskSpace/Cansu/HE_prediction_imp...   \n",
       "452  /media/cansu/DiskSpace/Cansu/HE_prediction_imp...   \n",
       "453  /media/cansu/DiskSpace/Cansu/HE_prediction_imp...   \n",
       "454  /media/cansu/DiskSpace/Cansu/HE_prediction_imp...   \n",
       "455  /media/cansu/DiskSpace/Cansu/HE_prediction_imp...   \n",
       "\n",
       "                                          fu_mask_path  \n",
       "0    /media/cansu/DiskSpace/Cansu/HE_prediction_imp...  \n",
       "1    /media/cansu/DiskSpace/Cansu/HE_prediction_imp...  \n",
       "2    /media/cansu/DiskSpace/Cansu/HE_prediction_imp...  \n",
       "3    /media/cansu/DiskSpace/Cansu/HE_prediction_imp...  \n",
       "4    /media/cansu/DiskSpace/Cansu/HE_prediction_imp...  \n",
       "..                                                 ...  \n",
       "451  /media/cansu/DiskSpace/Cansu/HE_prediction_imp...  \n",
       "452  /media/cansu/DiskSpace/Cansu/HE_prediction_imp...  \n",
       "453  /media/cansu/DiskSpace/Cansu/HE_prediction_imp...  \n",
       "454  /media/cansu/DiskSpace/Cansu/HE_prediction_imp...  \n",
       "455  /media/cansu/DiskSpace/Cansu/HE_prediction_imp...  \n",
       "\n",
       "[456 rows x 12 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_path = \"/media/cansu/DiskSpace/Cansu/HE_prediction_improved/data/data_with_both_basal_fu.csv\"\n",
    "\n",
    "# read all data \n",
    "df = pd.read_csv(data_path)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3f88c6f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "hospital_clinic_ids = \"/media/cansu/DiskSpace/Cansu/HE_prediction_improved/data/splits\"\n",
    "# hospital_true_ta_pseudoids = \"/media/cansu/DiskSpace/Cansu/HE_prediction_improved/data/splits/Hospital_Josep_Trueta_pseudo_ids.txt\"\n",
    "hospital_santpau_ids = \"/media/cansu/DiskSpace/Cansu/HE_prediction_improved/data/splits/Hospital_Sant_Pau_ids.txt\"\n",
    "hospital_vallhebron_ids = \"/media/cansu/DiskSpace/Cansu/HE_prediction_improved/data/splits/Hospital_Vall_Hebrón_ids.txt\"\n",
    "\n",
    "data_path = \"/media/cansu/DiskSpace/Cansu/HE_prediction_improved/data/data_with_both_basal_fu.csv\"\n",
    "\n",
    "# read all data \n",
    "df = pd.read_csv(data_path)\n",
    "\n",
    "# read txt files and convert to list\n",
    "def read_ids(txt_path):\n",
    "    with open(txt_path, \"r\") as f:\n",
    "        return [line.strip() for line in f if line.strip()]\n",
    "    \n",
    "# hospital_true_ta_pseudoids_list = read_ids(hospital_true_ta_pseudoids)\n",
    "hospital_santpau_ids_list = read_ids(hospital_santpau_ids)\n",
    "hospital_vallhebron_ids_list = read_ids(hospital_vallhebron_ids)\n",
    "all_ids =  hospital_santpau_ids_list + hospital_vallhebron_ids_list\n",
    "\n",
    "hos_clinic_path = \"/media/cansu/DiskSpace/Cansu/HE_prediction_improved/data/RAINS_vicorob/Hospital Clínic\"\n",
    "hos_josep_path = \"/media/cansu/DiskSpace/Cansu/HE_prediction_improved/data/RAINS_vicorob/Hospital Josep Trueta\"\n",
    "hos_san_pau_path = \"/media/cansu/DiskSpace/Cansu/HE_prediction_improved/data/RAINS_vicorob/Hospital Sant Pau\"\n",
    "hos_vall_hebron_path = \"/media/cansu/DiskSpace/Cansu/HE_prediction_improved/data/RAINS_vicorob/Hospital Vall Hebrón\"\n",
    "\n",
    "# create a dataframe with columns: patient_id,ct_ss_path,ct_nc_path,mask_path,label,volume,index\n",
    "data = []\n",
    "for pid in all_ids:\n",
    "    # if pid in hospital_true_ta_pseudoids_list:\n",
    "    #     hospital_path = hos_josep_path\n",
    "    if pid in hospital_santpau_ids_list:\n",
    "        hospital_path = hos_san_pau_path\n",
    "    elif pid in hospital_vallhebron_ids_list:\n",
    "        hospital_path = hos_vall_hebron_path\n",
    "    else:\n",
    "        continue\n",
    "\n",
    "    basal_path = os.path.join(hospital_path, pid, \"Basal\")\n",
    "    ct_ss_path = os.path.join(basal_path, \"CT_SS.nii.gz\")\n",
    "    ct_nc_path = os.path.join(basal_path, \"CT_NC.nii.gz\")\n",
    "    mask_path = os.path.join(basal_path, \"hematoma_mask_vicorob_reviewed_reoriented.nii.gz\")\n",
    "\n",
    "    # labels are from the df \n",
    "    label = df.loc[df[\"id\"] == pid, \"HE vicorob\"].values[0]\n",
    "\n",
    "    # give their int(pid) as index if it is integer or give a random index\n",
    "    index = int(pid)\n",
    "\n",
    "    data.append({\n",
    "        \"patient_id\": pid,\n",
    "        \"ct_ss_path\": ct_ss_path,\n",
    "        \"ct_nc_path\": ct_nc_path,\n",
    "        \"mask_path\": mask_path,\n",
    "        \"label\": label,\n",
    "        \"index\": index\n",
    "    })\n",
    "# svc to dataframe\n",
    "data_df = pd.DataFrame(data)\n",
    "data_df.to_csv(\"/media/cansu/DiskSpace/Cansu/HE_prediction_improved/data/metadata_clinic_trueta_santpau_vallhebron.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "936ab8b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using eff s\n",
      "Using focal loss\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200/200 [00:00<00:00, 97598.70it/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Exception thrown in SimpleITK ImageFileReader_Execute: /tmp/SimpleITK-build/ITK/Modules/IO/NIFTI/src/itkNiftiImageIO.cxx:2135:\nITK ERROR: ITK only supports orthonormal direction cosines.  No orthonormal definition found!",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_1941715/3843268059.py\u001b[0m in \u001b[0;36m?\u001b[0;34m()\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[0mdm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[0;31m# Read the test dataset from the datamodule\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m \u001b[0mtest_loader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtest_dataloader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m \u001b[0mimg_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[0mlabel_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/media/cansu/DiskSpace/Cansu/HE_prediction_improved/dataset.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    568\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mval_dataloader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    569\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muse_2d\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 570\u001b[0;31m             \u001b[0mslices\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpatient_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpatient_slice_numbers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_slices_from_subset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mval\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbasal_fu\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbasal_fu\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreturn_type\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# returns all the slices in the given set.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    571\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfilter_slices\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;31m# Return only the slices where the lesion is available\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    572\u001b[0m                 \u001b[0mmask_slices\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpatient_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpatient_slice_numbers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_slices_from_subset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mval\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'mask'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbasal_fu\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbasal_fu\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    573\u001b[0m                 filtered_slices, filtered_masks, filtered_labels, filtered_ids, filtered_slice_numbers = self.filter_segmented_slices(slices=slices, \n",
      "\u001b[0;32m/media/cansu/DiskSpace/Cansu/HE_prediction_improved/utils.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(dataset_subset, basal_fu, return_type)\u001b[0m\n\u001b[1;32m    126\u001b[0m     \u001b[0mmasks_fu\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m     \u001b[0mp_id\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdataset_subset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpatient_id\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    128\u001b[0m     \u001b[0mcount\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 130\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdataset_subset\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    131\u001b[0m         \u001b[0mimage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    132\u001b[0m         \u001b[0mpatient_id\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mp_id\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;31m#image.size(1) is the number of slices in the image.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/media/cansu/DiskSpace/miniconda3/envs/3dunet/lib/python3.10/site-packages/torch/utils/data/dataset.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m    353\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__getitem__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    354\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    355\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 356\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/media/cansu/DiskSpace/Cansu/HE_prediction_improved/dataset.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(self, idx)\u001b[0m\n",
      "\u001b[0;32m/media/cansu/DiskSpace/miniconda3/envs/3dunet/lib/python3.10/site-packages/SimpleITK/extra.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(fileName, outputPixelType, imageIO)\u001b[0m\n\u001b[1;32m    380\u001b[0m         \u001b[0mreader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSetFileNames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfileName\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    381\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    382\u001b[0m     \u001b[0mreader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSetImageIO\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimageIO\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    383\u001b[0m     \u001b[0mreader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSetOutputPixelType\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputPixelType\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 384\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mreader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mExecute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/media/cansu/DiskSpace/miniconda3/envs/3dunet/lib/python3.10/site-packages/SimpleITK/SimpleITK.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   8530\u001b[0m         \u001b[0mtype\u001b[0m \u001b[0mto\u001b[0m \u001b[0mbe\u001b[0m \u001b[0msame\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mIf\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mpixel\u001b[0m \u001b[0mtype\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mspecified\u001b[0m \u001b[0mthen\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mitk\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mConvertPixelBuffer\u001b[0m \u001b[0mwill\u001b[0m \u001b[0mbe\u001b[0m \u001b[0mused\u001b[0m \u001b[0mto\u001b[0m \u001b[0mconvert\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mpixels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   8531\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   8532\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   8533\u001b[0m         \"\"\"\n\u001b[0;32m-> 8534\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_SimpleITK\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mImageFileReader_Execute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m: Exception thrown in SimpleITK ImageFileReader_Execute: /tmp/SimpleITK-build/ITK/Modules/IO/NIFTI/src/itkNiftiImageIO.cxx:2135:\nITK ERROR: ITK only supports orthonormal direction cosines.  No orthonormal definition found!"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "import sys; sys.path.insert(0, os.path.abspath(\"../\"))\n",
    "from dataset import *\n",
    "from utils import *\n",
    "from model import *\n",
    "from model import ImageClassifier2\n",
    "import torch.utils.data as data\n",
    "import random\n",
    "import argparse\n",
    "import yaml\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "from tqdm import tqdm\n",
    "\n",
    "# chcekpoint for the best da_sy(5) model\n",
    "checkpoint_path = \"/media/cansu/DiskSpace/Cansu/HE_prediction_improved/checkpoints/org_da_sy_es5_hf05_repeat_2711_1_othermain/0/last_model-v1.ckpt\"\n",
    "\n",
    "model = ImageClassifier3.load_from_checkpoint(checkpoint_path)\n",
    "model.eval().cuda(device=0)\n",
    "\n",
    "unseen_neg_samples = pd.read_csv(\"/media/cansu/DiskSpace/Cansu/HE_prediction_improved/data/metadata_clinic_trueta_santpau_vallhebron.csv\")\n",
    "\n",
    "X_train =[]\n",
    "y_train = []\n",
    "X_val = []\n",
    "X_test = []\n",
    "y_val = []\n",
    "y_test = []\n",
    "\n",
    "md_path = \"/media/cansu/DiskSpace/Cansu/HE_prediction_improved/data/metadata_clinic_trueta_santpau_vallhebron.csv\"\n",
    "\n",
    "for i in tqdm(range(len(unseen_neg_samples))):\n",
    "    # add the index numbers of the unseen negative samples to the X_test list\n",
    "    X_test.append(unseen_neg_samples[\"index\"].tolist()[i])\n",
    "    y_test.append(unseen_neg_samples[\"label\"].tolist()[i])\n",
    "\n",
    "indexes = X_train, X_val, X_test, y_train, y_val, y_test\n",
    "\n",
    "dm = HEPredDataModule(split_indexes=indexes, \n",
    "                        filter_slices=True,\n",
    "                        mask=True, \n",
    "                        batch_size=1, \n",
    "                        num_workers=8, \n",
    "                        use_2d=True, \n",
    "                        return_type='image',\n",
    "                        under_sampling=False,\n",
    "                        over_sampling=False,\n",
    "                        threshold=0.5,\n",
    "                        md_path=md_path,\n",
    "                        basal_fu=False,\n",
    "                        roi = False, problem = \"prediction\",\n",
    "                        image_size = 512,\n",
    "                        roi_size=512, lesion=False,\n",
    "                        test_type = \"t5\",\n",
    "                        apply_hflip = False,\n",
    "                        apply_affine = False,\n",
    "                        apply_gaussian_blur= False,\n",
    "                        affine_degree=10,\n",
    "                        affine_translate=0,\n",
    "                        affine_scale=1.0,\n",
    "                        affine_shear=0, \n",
    "                        hflip_p = 0.5, affine_p = 0.5,)\n",
    "dm.setup()\n",
    "\n",
    "# Read the test dataset from the datamodule\n",
    "\n",
    "test_loader = dm.test_dataloader()\n",
    "\n",
    "img_list = []\n",
    "label_list = []\n",
    "id_list = []\n",
    "snum_list = []\n",
    "\n",
    "for batch in test_loader:\n",
    "    image, label, id, snum = batch\n",
    "    img_list.append(image)\n",
    "    label_list.append(label)\n",
    "    id_list.append(id)\n",
    "    snum_list.append(snum)\n",
    "\n",
    "probabilities = []\n",
    "for i in range(len(id_list)):\n",
    "    with torch.no_grad():\n",
    "        y_hat = model(img_list[i].cuda(device=0))\n",
    "        probabilities.append(y_hat.squeeze().cpu().numpy().tolist())\n",
    "\n",
    "#make a dataframe with the id, snum and probabilities\n",
    "df = pd.DataFrame(list(zip(id_list, snum_list, probabilities)), columns = [\"id\", \"snum\", \"probabilities\"])\n",
    "# change the id column to integer\n",
    "df[\"id\"] = df[\"id\"].astype(int)\n",
    "\n",
    "# for each patient id take the probablity average of the slices\n",
    "df_patient_averages = pd.DataFrame(columns = [\"id\", \"average_probabilities\"])\n",
    "\n",
    "id_dict = {}\n",
    "for i in range(len(df)):\n",
    "    current_id = df[\"id\"][i]\n",
    "    probability = df[\"probabilities\"][i]\n",
    "    if current_id in id_dict:\n",
    "        id_dict[current_id].append(probability)\n",
    "    else:\n",
    "        id_dict[current_id] = [probability]\n",
    "\n",
    "# for each id, take the average of the probablities\n",
    "for id, probablities in id_dict.items():\n",
    "    average = sum(probablities) / len(probablities)\n",
    "    # DataFrame.append is removed in recent pandas versions. Use .loc to add a new row instead.\n",
    "    df_patient_averages.loc[len(df_patient_averages)] = {\"id\": id, \"average_probabilities\": average}\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1e37d9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add another column as predicted labels where if the probablity is greater than 0.5, label is 1, else 0\n",
    "df_patient_averages[\"predicted_labels\"] = df_patient_averages[\"average_probabilities\"].apply(lambda x: 1 if x > 0.5 else 0)\n",
    "# add the actual labels to the df_patient_averages where all of them are 0's\n",
    "df_patient_averages[\"actual_labels\"] = 0"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "he_pred",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
